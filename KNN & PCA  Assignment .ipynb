{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9654ff96-a464-4c24-8f70-94298a3e29d3",
   "metadata": {},
   "source": [
    "# KNN & PCA Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a784992-2227-4974-8009-8c198cc9094f",
   "metadata": {},
   "source": [
    "Q 1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
    "\n",
    "->  K-Nearest Neighbors (KNN) is a simple but powerful supervised learning algorithm that makes predictions based on the similarity between data points. Instead of building a mathematical model during training, KNN stores the entire dataset and makes decisions only when a new sample is given. This is why it is called a lazy learner.\n",
    "\n",
    "In classification, KNN finds the “k” closest neighbors and assigns the class that appears most frequently among them. It assumes that points located closer to one another are more likely to belong to the same category.\n",
    "\n",
    "In regression, KNN predicts continuous values by averaging the output values of the k nearest neighbors. Because the algorithm depends heavily on the distance between points, proper scaling is very important. Overall, KNN works by comparing distances and making predictions based on majority voting or average values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3851de-f142-4438-8a29-8bfef7413a72",
   "metadata": {},
   "source": [
    "Q 2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "\n",
    "->  The Curse of Dimensionality refers to the problems that arise when the number of features in a dataset becomes very large. In high-dimensional space, data points become sparse and distances between them become less meaningful. As a result, even points that are supposed to be close appear far apart mathematically.\n",
    "\n",
    "Since KNN depends directly on distance calculations, this causes the algorithm to make poor decisions. The model becomes slow, inaccurate, and requires much more data to work correctly.\n",
    "\n",
    "This is why dimensionality reduction techniques like PCA are often applied before using KNN, especially when datasets contain a large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e0fb1-1000-4557-8ff6-fc22e7d0d224",
   "metadata": {},
   "source": [
    "Q 3. What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
    "\n",
    "->  Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original high-dimensional data into a smaller number of new variables called principal components. These components capture as much information (variance) as possible while reducing redundancy among features.\n",
    "\n",
    "The key difference between PCA and feature selection is that PCA creates new features by combining existing ones, whereas feature selection simply chooses the most important existing features without altering them.\n",
    "\n",
    "PCA is very useful for simplifying complex datasets, improving model performance, and reducing noise. It also helps in visualizing high-dimensional data more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305cb1f6-bd0a-4d72-894d-349e3764b17d",
   "metadata": {},
   "source": [
    "Q 4. What are eigenvalues and eigenvectors in PCA, and why are they important? \n",
    "\n",
    "->  In PCA, eigenvalues and eigenvectors come from the covariance matrix of the dataset. Eigenvectors represent the directions in which the data varies the most, while eigenvalues represent how much variance lies along each direction.\n",
    "\n",
    "Eigenvectors determine the orientation of the principal components, and eigenvalues determine their importance. Components with high eigenvalues carry more information about the dataset.\n",
    "\n",
    "PCA ranks components based on their eigenvalues, helping us decide how many components to keep. Without eigenvalues and eigenvectors, PCA would not be able to identify meaningful patterns and reduce dimensions properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c2f3c-7840-48cb-9274-9f1e6b7297bf",
   "metadata": {},
   "source": [
    "Q 5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
    "\n",
    "->  KNN works best when the number of features is small and well-scaled, while PCA reduces the number of features by extracting only the most important components. When PCA is applied before KNN, it removes noise, eliminates redundant information, and makes distances more meaningful.\n",
    "\n",
    "This improves KNN’s accuracy, reduces overfitting, and speeds up computation. Therefore, using PCA followed by KNN creates a strong and efficient pipeline, especially for datasets with many correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d869686-e111-408c-ba61-f87574680c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: \n",
    "# Use the Wine Dataset from sklearn.datasets.load_wine()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10fd54-965f-4af6-a932-586e232c394a",
   "metadata": {},
   "source": [
    "Q 6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
    "\n",
    "-> Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2ef2d5-5c2f-4c43-88f8-e5825d13757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Without Scaling: 0.7222222222222222\n",
      "Accuracy With Scaling: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Python Code\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Without Scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scale.fit(X_train, y_train)\n",
    "acc_no_scale = accuracy_score(y_test, knn_no_scale.predict(X_test))\n",
    "\n",
    "# With Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "knn_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scale.fit(X_train_s, y_train_s)\n",
    "acc_scale = accuracy_score(y_test_s, knn_scale.predict(X_test_s))\n",
    "\n",
    "print(\"Accuracy Without Scaling:\", acc_no_scale)\n",
    "print(\"Accuracy With Scaling:\", acc_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b11b7-b4db-4503-95b8-f5bfb3ef501b",
   "metadata": {},
   "source": [
    "Q 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
    "\n",
    "-> Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e64430-1d5a-4d03-889d-a9393fefc259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
      " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
      " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
      " 8.25392788e-08]\n"
     ]
    }
   ],
   "source": [
    "# Python Code\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA Model\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d46639-6c48-4d77-8758-6ddf93b9122e",
   "metadata": {},
   "source": [
    "Q 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
    "\n",
    "-> Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef2e28ec-8626-4525-a7c2-2ea14c3537ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with PCA (2 components): 0.7222222222222222\n",
      "Original Accuracy (Scaled): 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Python Code\n",
    "# PCA with 2 components\n",
    "pca_2 = PCA(n_components=2)\n",
    "X_pca = pca_2.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
    "\n",
    "print(\"Accuracy with PCA (2 components):\", acc_pca)\n",
    "print(\"Original Accuracy (Scaled):\", acc_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68195d7-75ab-4d84-9a88-86f7c17682a3",
   "metadata": {},
   "source": [
    "Q 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
    "\n",
    "->  Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3f79a4-5374-46dc-9745-6c114ad2016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Accuracy: 0.9444444444444444\n",
      "Manhattan Accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Python Code\n",
    "# Euclidean\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train_s, y_train_s)\n",
    "acc_eu = accuracy_score(y_test_s, knn_euclidean.predict(X_test_s))\n",
    "\n",
    "# Manhattan\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train_s, y_train_s)\n",
    "acc_man = accuracy_score(y_test_s, knn_manhattan.predict(X_test_s))\n",
    "\n",
    "print(\"Euclidean Accuracy:\", acc_eu)\n",
    "print(\"Manhattan Accuracy:\", acc_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496e94e-ab25-417e-9a9e-c70d283a721f",
   "metadata": {},
   "source": [
    "Q 10. You are working with a high-dimensional gene expression dataset to \n",
    "classify patients with different types of cancer.\n",
    "\n",
    "Due to the large number of features and a small number of samples, traditional models \n",
    "overfit.\n",
    "\n",
    "Explain how you would: \n",
    "\n",
    "● Use PCA to reduce dimensionality \n",
    "\n",
    "● Decide how many components to keep \n",
    "\n",
    "● Use KNN for classification post-dimensionality reduction\n",
    "\n",
    "● Evaluate the model \n",
    "\n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world \n",
    "biomedical data\n",
    "\n",
    "->  In high-dimensional gene expression datasets, the number of features is extremely large, while the number of samples is very small. This creates a major challenge because most traditional machine learning models start overfitting—they memorize noise instead of learning true patterns. To handle this problem effectively, the first step I would take is applying Principal Component Analysis (PCA). PCA reduces dimensionality by converting thousands of gene features into a smaller set of principal components that still capture most of the important biological variation.\n",
    "\n",
    "To decide how many components to keep, I would look at the explained variance ratio and choose enough components to preserve around 90–95% of the total information. By selecting components based on variance, we ensure that noise and irrelevant gene expressions are removed, while key biological signals are retained. This step not only makes the dataset smaller but also improves the stability of further classification.\n",
    "\n",
    "After dimensionality reduction, I would apply the K-Nearest Neighbors (KNN) algorithm for classification. KNN works well in lower-dimensional space because distance relationships become more meaningful once redundant features are removed. Using PCA before KNN ensures that the distance calculations are not affected by irrelevant gene expressions. I would also tune the value of ‘k’ to achieve the best accuracy.\n",
    "\n",
    "For evaluating the model, I would use accuracy, confusion matrix, and cross-validation to check if the model performs consistently across different folds. Cross-validation is especially important in biomedical datasets where sample sizes are very small. This helps confirm that the model is generalizing and not overfitting to specific patients.\n",
    "\n",
    "This PCA + KNN pipeline is highly suitable for real-world biomedical data because it reduces complexity, removes noise, and allows KNN to make reliable predictions. It also creates a more interpretable system for stakeholders. PCA helps visualize patterns in genetic profiles, and KNN provides clear classification outcomes. Together, they offer a robust, scientific, and practical solution for cancer prediction problems where feature count is extremely high and accuracy is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61594cd7-b0b5-4993-b83d-cdd3442423bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
