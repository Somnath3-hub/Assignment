{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50134f4-a81b-4fd3-8254-4afa5cbf592d",
   "metadata": {},
   "source": [
    "# Supervised Classification: Decision Trees, SVM, and Naive Bayes Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48b23a-83e3-4372-b8f2-ef446f767568",
   "metadata": {},
   "source": [
    " Q 1.  What is Information Gain, and how is it used in Decision Trees?\n",
    "\n",
    " ->   Information Gain is a concept from information theory used to measure how well a particular feature separates the data into different classes. In simple words, it tells us how much “information” a feature adds when we split the dataset based on that feature. A high Information Gain means the feature helps in making the data more pure and less mixed.\n",
    "\n",
    "In Decision Trees, Information Gain is used for selecting the best attribute at every step of tree construction. The algorithm checks all the available features and calculates their Information Gain. The feature that gives the highest gain becomes the root or the next node. This process continues until the tree reaches the stopping condition. Because of this, Information Gain helps the tree make better decisions and increases accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf0a2e-2b77-4631-8a31-30fa8edaeced",
   "metadata": {},
   "source": [
    "Q 2. What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    "->   Gini Impurity and Entropy are two main impurity measures used in Decision Trees. Both measure how mixed the data is, but they work in slightly different ways.\n",
    "\n",
    "Gini Impurity calculates the probability that a randomly chosen element will be misclassified. It is simpler and faster to compute, making it suitable for large datasets.\n",
    "\n",
    "Entropy comes from information theory and measures the amount of randomness or disorder in the data. It is mathematically heavier but gives more precise splits when classes are very uneven.\n",
    "\n",
    "Gini Impurity is preferred in CART algorithm because of its speed, while Entropy is used in ID3 and C4.5 algorithms for more detailed evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167d6bf-0940-4e5b-aa7d-a4917de5a219",
   "metadata": {},
   "source": [
    "Q 3. What is Pre-Pruning in Decision Trees?\n",
    "\n",
    "->   Pre-Pruning is a technique used to stop a Decision Tree from growing too large during its training process. Instead of allowing the tree to expand fully and then cutting unnecessary branches later, pre-pruning stops the splitting early if the algorithm detects that further splits will not improve accuracy.\n",
    "\n",
    "Common pre-pruning methods include setting a maximum depth, minimum samples for split, or minimum leaf size. This prevents the model from overfitting and helps in creating a simpler, more general model. Because of this, pre-pruning saves time and improves performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d6bd-9581-4397-abf8-b5d8c72ac8f6",
   "metadata": {},
   "source": [
    "Q 4. Write a Python program to train a Decision Tree Classifier using Gini \n",
    "Impurity as the criterion and print the feature importances (practical). \n",
    "    \n",
    "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_. \n",
    "\n",
    "->  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf7d9b36-0ba6-4b05-8f88-d59a58ec1aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "[0.01333333 0.01333333 0.55072262 0.42261071]\n"
     ]
    }
   ],
   "source": [
    "# python Code\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train Decision Tree using Gini\n",
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78c05d-440e-4a95-b18a-6b59f3749902",
   "metadata": {},
   "source": [
    "Q 5. What is a Support Vector Machine (SVM)? \n",
    "\n",
    "->  Support Vector Machine is a supervised machine learning model used for classification and regression. Its main idea is to find the best possible boundary, called a hyperplane, that separates different classes with maximum margin. The datapoints closest to the boundary are called support vectors, and they play a key role in defining the decision line.\n",
    "\n",
    "SVM is powerful because it works well even when the number of features is large, and it can handle both linear and non-linear data. Because of its strong mathematical foundation, SVM gives very high accuracy on many real-world classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fa0b0-d45c-4c49-9b89-c1995fdb1b67",
   "metadata": {},
   "source": [
    "Q 6. What is the Kernel Trick in SVM? \n",
    "\n",
    "->  The Kernel Trick is a mathematical method that allows SVM to perform classification on non-linear data. Instead of transforming the data manually to a higher-dimensional space, the kernel function automatically computes relationships in that space.\n",
    "\n",
    "Common kernels include Linear, Polynomial, and RBF (Gaussian). With the kernel trick, SVM can create complex decision boundaries without increasing the computational cost too much. This makes SVM extremely flexible and powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591b29a-67d7-4653-aa62-4db704efad7c",
   "metadata": {},
   "source": [
    "Q 7. Write a Python program to train two SVM classifiers with Linear and RBF \n",
    "kernels on the Wine dataset, then compare their accuracies. \n",
    "\n",
    "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting \n",
    "on the same dataset.\n",
    "\n",
    "->  Python Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4c14a1-2d52-403f-a073-fe4ca32944c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel Accuracy: 0.9814814814814815\n",
      "RBF Kernel Accuracy: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Linear SVM\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
    "\n",
    "# RBF SVM\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
    "\n",
    "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
    "print(\"RBF Kernel Accuracy:\", acc_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b2324-aa10-47a2-a31d-3b5e3d1e996c",
   "metadata": {},
   "source": [
    "Q 8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "\n",
    "->  Naïve Bayes is a probabilistic classification algorithm based on Bayes' Theorem. It predicts the class of a sample by calculating the probability of each class given the input features. It works extremely well for text classification, spam filtering, and sentiment analysis.\n",
    "\n",
    "It is called “Naïve” because it assumes that all features are independent of each other, which is rarely true in real life. However, even with this simple assumption, Naïve Bayes performs surprisingly well and is fast to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc10952-f23b-4d87-ab86-2feb248addf8",
   "metadata": {},
   "source": [
    "Q 9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve \n",
    "Bayes, and Bernoulli Naïve Bayes \n",
    "\n",
    "->  Gaussian Naïve Bayes: Used when features are continuous and follow a normal distribution. Works well in datasets like Iris and medical measurements.\n",
    "\n",
    "Multinomial Naïve Bayes: Used for discrete counts, especially in text data such as word frequency or term counts in documents.\n",
    "\n",
    "Bernoulli Naïve Bayes: Used when features are binary (0 or 1). Useful in cases like “word present or not present” in text data.\n",
    "\n",
    "Each type fits different situations based on the nature of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b90b6-100e-42f6-8805-d0f5dc79e4e4",
   "metadata": {},
   "source": [
    "Q 10. Breast Cancer Dataset \n",
    "\n",
    "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer \n",
    "dataset and evaluate accuracy. \n",
    "    \n",
    "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from \n",
    "sklearn.datasets.\n",
    "\n",
    "->  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae952-3f8b-48e6-bab6-03c99424843f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
